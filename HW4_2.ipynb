{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "HW4_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seokyoungchoi/AI-python-connect/blob/master/HW4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUe50u9psBJw",
        "colab_type": "text"
      },
      "source": [
        "Since I'm using Colab, I added some lines to get the data files from google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JBW8heoqM3C",
        "colab_type": "code",
        "outputId": "a93920a9-e654-4b61-9d7f-e29b38d3e24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from __future__ import print_function #Python 2/3 compatibility for print statements\n",
        "import pandas\n",
        "pandas.set_option('display.max_colwidth', 170) #widen pandas rows display\n",
        "\n",
        "%matplotlib inline\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "\n",
        "# Ignore  the warnings\n",
        "import warnings\n",
        "#warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1-cIYbbqM3N",
        "colab_type": "code",
        "outputId": "c928d562-fbf6-4a0a-9f5c-d22299111e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "'''Load the dataset'''\n",
        "\n",
        "train_sents = pandas.read_csv('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/dataset/example_train_brown_corpus.csv', encoding='utf-8')\n",
        "\n",
        "#Get the word tokens and tags into a readable list format\n",
        "train_sents['Tokenized_Sentence'] = train_sents['Tokenized_Sentence'].apply(lambda sent: sent.lower().split(\"\\t\"))\n",
        "train_sents['Tagged_Sentence'] = train_sents['Tagged_Sentence'].apply(lambda sent: sent.split(\"\\t\"))\n",
        "\n",
        "train_sents[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tagged_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[the, fulton, county, grand, jury, said, friday, an, investigation, of, atlanta's, recent, primary, election, produced, ``, no, evidence, '', that, any, irregularitie...</td>\n",
              "      <td>[DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, NOUN, ADP, NOUN, ADJ, NOUN, NOUN, VERB, ., DET, NOUN, ., ADP, DET, NOUN, VERB, NOUN, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[the, jury, further, said, in, term-end, presentments, that, the, city, executive, committee, ,, which, had, over-all, charge, of, the, election, ,, ``, deserves, the...</td>\n",
              "      <td>[DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, DET, NOUN, ADJ, NOUN, ., DET, VERB, ADJ, NOUN, ADP, DET, NOUN, ., ., VERB, DET, NOUN, CONJ, NOUN, ADP, DET, NOUN, ADP, NO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[the, september-october, term, jury, had, been, charged, by, fulton, superior, court, judge, durwood, pye, to, investigate, reports, of, possible, ``, irregularities,...</td>\n",
              "      <td>[DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP, NOUN, ADJ, NOUN, NOUN, NOUN, NOUN, PRT, VERB, NOUN, ADP, ADJ, ., NOUN, ., ADP, DET, ADJ, NOUN, DET, VERB, VERB, ADP, NO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[``, only, a, relative, handful, of, such, reports, was, received, '', ,, the, jury, said, ,, ``, considering, the, widespread, interest, in, the, election, ,, the, n...</td>\n",
              "      <td>[., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB, VERB, ., ., DET, NOUN, VERB, ., ., ADP, DET, ADJ, NOUN, ADP, DET, NOUN, ., DET, NOUN, ADP, NOUN, CONJ, DET, NOUN, ADP, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[the, jury, said, it, did, find, that, many, of, georgia's, registration, and, election, laws, ``, are, outmoded, or, inadequate, and, often, ambiguous, '', .]</td>\n",
              "      <td>[DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ADP, NOUN, NOUN, CONJ, NOUN, NOUN, ., VERB, ADJ, CONJ, ADJ, CONJ, ADV, ADJ, ., .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[it, recommended, that, fulton, legislators, act, ``, to, have, these, laws, studied, and, revised, to, the, end, of, modernizing, and, improving, them, '', .]</td>\n",
              "      <td>[PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VERB, DET, NOUN, VERB, CONJ, VERB, ADP, DET, NOUN, ADP, VERB, CONJ, VERB, PRON, ., .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[the, grand, jury, commented, on, a, number, of, other, topics, ,, among, them, the, atlanta, and, fulton, county, purchasing, departments, which, it, said, ``, are, ...</td>\n",
              "      <td>[DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, ADJ, NOUN, ., ADP, PRON, DET, NOUN, CONJ, NOUN, NOUN, VERB, NOUN, DET, PRON, VERB, ., VERB, ADV, VERB, CONJ, VERB, ADV, VE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[merger, proposed]</td>\n",
              "      <td>[NOUN, VERB]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[however, ,, the, jury, said, it, believes, ``, these, two, offices, should, be, combined, to, achieve, greater, efficiency, and, reduce, the, cost, of, administratio...</td>\n",
              "      <td>[ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, NUM, NOUN, VERB, VERB, VERB, PRT, VERB, ADJ, NOUN, CONJ, VERB, DET, NOUN, ADP, NOUN, ., .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[the, city, purchasing, department, ,, the, jury, said, ,, ``, is, lacking, in, experienced, clerical, personnel, as, a, result, of, city, personnel, policies, '', .]</td>\n",
              "      <td>[DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, ., ., VERB, VERB, ADP, VERB, ADJ, NOUN, ADP, DET, NOUN, ADP, NOUN, NOUN, NOUN, ., .]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                          Tokenized_Sentence                                                                                                                                                            Tagged_Sentence\n",
              "0  [the, fulton, county, grand, jury, said, friday, an, investigation, of, atlanta's, recent, primary, election, produced, ``, no, evidence, '', that, any, irregularitie...                                      [DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, NOUN, ADP, NOUN, ADJ, NOUN, NOUN, VERB, ., DET, NOUN, ., ADP, DET, NOUN, VERB, NOUN, .]\n",
              "1  [the, jury, further, said, in, term-end, presentments, that, the, city, executive, committee, ,, which, had, over-all, charge, of, the, election, ,, ``, deserves, the...  [DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, DET, NOUN, ADJ, NOUN, ., DET, VERB, ADJ, NOUN, ADP, DET, NOUN, ., ., VERB, DET, NOUN, CONJ, NOUN, ADP, DET, NOUN, ADP, NO...\n",
              "2  [the, september-october, term, jury, had, been, charged, by, fulton, superior, court, judge, durwood, pye, to, investigate, reports, of, possible, ``, irregularities,...  [DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP, NOUN, ADJ, NOUN, NOUN, NOUN, NOUN, PRT, VERB, NOUN, ADP, ADJ, ., NOUN, ., ADP, DET, ADJ, NOUN, DET, VERB, VERB, ADP, NO...\n",
              "3  [``, only, a, relative, handful, of, such, reports, was, received, '', ,, the, jury, said, ,, ``, considering, the, widespread, interest, in, the, election, ,, the, n...  [., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB, VERB, ., ., DET, NOUN, VERB, ., ., ADP, DET, ADJ, NOUN, ADP, DET, NOUN, ., DET, NOUN, ADP, NOUN, CONJ, DET, NOUN, ADP, ...\n",
              "4            [the, jury, said, it, did, find, that, many, of, georgia's, registration, and, election, laws, ``, are, outmoded, or, inadequate, and, often, ambiguous, '', .]                                            [DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ADP, NOUN, NOUN, CONJ, NOUN, NOUN, ., VERB, ADJ, CONJ, ADJ, CONJ, ADV, ADJ, ., .]\n",
              "5            [it, recommended, that, fulton, legislators, act, ``, to, have, these, laws, studied, and, revised, to, the, end, of, modernizing, and, improving, them, '', .]                                          [PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VERB, DET, NOUN, VERB, CONJ, VERB, ADP, DET, NOUN, ADP, VERB, CONJ, VERB, PRON, ., .]\n",
              "6  [the, grand, jury, commented, on, a, number, of, other, topics, ,, among, them, the, atlanta, and, fulton, county, purchasing, departments, which, it, said, ``, are, ...  [DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, ADJ, NOUN, ., ADP, PRON, DET, NOUN, CONJ, NOUN, NOUN, VERB, NOUN, DET, PRON, VERB, ., VERB, ADV, VERB, CONJ, VERB, ADV, VE...\n",
              "7                                                                                                                                                         [merger, proposed]                                                                                                                                                               [NOUN, VERB]\n",
              "8  [however, ,, the, jury, said, it, believes, ``, these, two, offices, should, be, combined, to, achieve, greater, efficiency, and, reduce, the, cost, of, administratio...                                   [ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, NUM, NOUN, VERB, VERB, VERB, PRT, VERB, ADJ, NOUN, CONJ, VERB, DET, NOUN, ADP, NOUN, ., .]\n",
              "9     [the, city, purchasing, department, ,, the, jury, said, ,, ``, is, lacking, in, experienced, clerical, personnel, as, a, result, of, city, personnel, policies, '', .]                                           [DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, ., ., VERB, VERB, ADP, VERB, ADJ, NOUN, ADP, DET, NOUN, ADP, NOUN, NOUN, NOUN, ., .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY8OBf-IqM3a",
        "colab_type": "code",
        "outputId": "229468bb-caa2-459d-8081-46a9c5a30609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "'''Create a lexicon for the words in the sentences as well as the tags'''\n",
        "\n",
        "import pickle\n",
        "\n",
        "def make_lexicon(token_seqs, min_freq=1):\n",
        "    # First, count how often each word appears in the text.\n",
        "    token_counts = {}\n",
        "    for seq in token_seqs:\n",
        "        for token in seq:\n",
        "            if token in token_counts:\n",
        "                token_counts[token] += 1\n",
        "            else:\n",
        "                token_counts[token] = 1\n",
        "\n",
        "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
        "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
        "    # Indices start at 2. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
        "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
        "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
        "    lexicon_size = len(lexicon)\n",
        "\n",
        "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
        "    print(dict(list(lexicon.items())[:20]))\n",
        "    \n",
        "    return lexicon\n",
        "\n",
        "print(\"WORDS:\")\n",
        "words_lexicon = make_lexicon(train_sents['Tokenized_Sentence'])\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/words_lexicon.pkl', 'wb') as f: #save the tags lexicon by pickling it\n",
        "    pickle.dump(words_lexicon, f)\n",
        "\n",
        "print(\"TAGS:\")\n",
        "tags_lexicon = make_lexicon(train_sents['Tagged_Sentence'])\n",
        "with open('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/tags_lexicon.pkl', 'wb') as f: #save the words lexicon by pickling it\n",
        "    pickle.dump(tags_lexicon, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WORDS:\n",
            "LEXICON SAMPLE (812 total items):\n",
            "{'the': 2, 'fulton': 3, 'county': 4, 'grand': 5, 'jury': 6, 'said': 7, 'friday': 8, 'an': 9, 'investigation': 10, 'of': 11, \"atlanta's\": 12, 'recent': 13, 'primary': 14, 'election': 15, 'produced': 16, '``': 17, 'no': 18, 'evidence': 19, \"''\": 20, 'that': 21}\n",
            "TAGS:\n",
            "LEXICON SAMPLE (12 total items):\n",
            "{'DET': 2, 'NOUN': 3, 'ADJ': 4, 'VERB': 5, 'ADP': 6, '.': 7, 'ADV': 8, 'CONJ': 9, 'PRT': 10, 'PRON': 11, 'NUM': 12, '<UNK>': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVdOYQkMqM3l",
        "colab_type": "code",
        "outputId": "dd007a27-05eb-4ea5-c15c-ade1f46412b9",
        "colab": {}
      },
      "source": [
        "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
        "\n",
        "def get_lexicon_lookup(lexicon):\n",
        "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
        "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
        "    print(dict(list(lexicon_lookup.items())[:20]))\n",
        "    return lexicon_lookup\n",
        "\n",
        "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEXICON LOOKUP SAMPLE:\n",
            "{2: 'DET', 3: 'NOUN', 4: 'ADJ', 5: 'VERB', 6: 'ADP', 7: '.', 8: 'ADV', 9: 'CONJ', 10: 'PRT', 11: 'PRON', 12: 'NUM', 1: '<UNK>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8jEEgNdqM3u",
        "colab_type": "code",
        "outputId": "3ead1d96-4242-4c5c-e506-f3ee68f2e289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "source": [
        "def tokens_to_idxs(token_seqs, lexicon):\n",
        "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
        "                                                                     for token_seq in token_seqs]\n",
        "    return idx_seqs\n",
        "\n",
        "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['Tokenized_Sentence'], words_lexicon)\n",
        "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['Tagged_Sentence'], tags_lexicon)\n",
        "train_sents[['Tokenized_Sentence', 'Sentence_Idxs', 'Tagged_Sentence', 'Tag_Idxs']][:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Sentence_Idxs</th>\n",
              "      <th>Tagged_Sentence</th>\n",
              "      <th>Tag_Idxs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[the, fulton, county, grand, jury, said, friday, an, investigation, of, atlanta's, recent, primary, election, produced, ``, no, evidence, '', that, any, irregularitie...</td>\n",
              "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]</td>\n",
              "      <td>[DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, NOUN, ADP, NOUN, ADJ, NOUN, NOUN, VERB, ., DET, NOUN, ., ADP, DET, NOUN, VERB, NOUN, .]</td>\n",
              "      <td>[2, 3, 3, 4, 3, 5, 3, 2, 3, 6, 3, 4, 3, 3, 5, 7, 2, 3, 7, 6, 2, 3, 5, 3, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[the, jury, further, said, in, term-end, presentments, that, the, city, executive, committee, ,, which, had, over-all, charge, of, the, election, ,, ``, deserves, the...</td>\n",
              "      <td>[2, 6, 27, 7, 28, 29, 30, 21, 2, 31, 32, 33, 34, 35, 36, 37, 38, 11, 2, 15, 34, 17, 39, 2, 40, 41, 42, 11, 2, 31, 11, 43, 20, 44, 2, 45, 28, 35, 2, 15, 46, 47, 26]</td>\n",
              "      <td>[DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, DET, NOUN, ADJ, NOUN, ., DET, VERB, ADJ, NOUN, ADP, DET, NOUN, ., ., VERB, DET, NOUN, CONJ, NOUN, ADP, DET, NOUN, ADP, NO...</td>\n",
              "      <td>[2, 3, 8, 5, 6, 3, 3, 6, 2, 3, 4, 3, 7, 2, 5, 4, 3, 6, 2, 3, 7, 7, 5, 2, 3, 9, 3, 6, 2, 3, 6, 3, 7, 6, 2, 3, 6, 2, 2, 3, 5, 5, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[the, september-october, term, jury, had, been, charged, by, fulton, superior, court, judge, durwood, pye, to, investigate, reports, of, possible, ``, irregularities,...</td>\n",
              "      <td>[2, 48, 49, 6, 36, 50, 51, 52, 3, 53, 54, 55, 56, 57, 58, 59, 60, 11, 61, 17, 23, 20, 28, 2, 62, 14, 35, 46, 63, 52, 64, 65, 66, 67, 26]</td>\n",
              "      <td>[DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP, NOUN, ADJ, NOUN, NOUN, NOUN, NOUN, PRT, VERB, NOUN, ADP, ADJ, ., NOUN, ., ADP, DET, ADJ, NOUN, DET, VERB, VERB, ADP, NO...</td>\n",
              "      <td>[2, 3, 3, 3, 5, 5, 5, 6, 3, 4, 3, 3, 3, 3, 10, 5, 3, 6, 4, 7, 3, 7, 6, 2, 4, 3, 2, 5, 5, 6, 3, 3, 3, 3, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[``, only, a, relative, handful, of, such, reports, was, received, '', ,, the, jury, said, ,, ``, considering, the, widespread, interest, in, the, election, ,, the, n...</td>\n",
              "      <td>[17, 68, 69, 70, 71, 11, 72, 60, 46, 73, 20, 34, 2, 6, 7, 34, 17, 74, 2, 75, 76, 28, 2, 15, 34, 2, 77, 11, 78, 41, 2, 79, 11, 80, 31, 20, 26]</td>\n",
              "      <td>[., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB, VERB, ., ., DET, NOUN, VERB, ., ., ADP, DET, ADJ, NOUN, ADP, DET, NOUN, ., DET, NOUN, ADP, NOUN, CONJ, DET, NOUN, ADP, ...</td>\n",
              "      <td>[7, 8, 2, 4, 3, 6, 4, 3, 5, 5, 7, 7, 2, 3, 5, 7, 7, 6, 2, 4, 3, 6, 2, 3, 7, 2, 3, 6, 3, 9, 2, 3, 6, 2, 3, 7, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[the, jury, said, it, did, find, that, many, of, georgia's, registration, and, election, laws, ``, are, outmoded, or, inadequate, and, often, ambiguous, '', .]</td>\n",
              "      <td>[2, 6, 7, 81, 82, 83, 21, 84, 11, 85, 86, 41, 15, 87, 17, 88, 89, 90, 91, 41, 92, 93, 20, 26]</td>\n",
              "      <td>[DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ADP, NOUN, NOUN, CONJ, NOUN, NOUN, ., VERB, ADJ, CONJ, ADJ, CONJ, ADV, ADJ, ., .]</td>\n",
              "      <td>[2, 3, 5, 11, 5, 5, 6, 4, 6, 3, 3, 9, 3, 3, 7, 5, 4, 9, 4, 9, 8, 4, 7, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[it, recommended, that, fulton, legislators, act, ``, to, have, these, laws, studied, and, revised, to, the, end, of, modernizing, and, improving, them, '', .]</td>\n",
              "      <td>[81, 94, 21, 3, 95, 96, 17, 58, 97, 98, 87, 99, 41, 100, 58, 2, 101, 11, 102, 41, 103, 104, 20, 26]</td>\n",
              "      <td>[PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VERB, DET, NOUN, VERB, CONJ, VERB, ADP, DET, NOUN, ADP, VERB, CONJ, VERB, PRON, ., .]</td>\n",
              "      <td>[11, 5, 6, 3, 3, 5, 7, 10, 5, 2, 3, 5, 9, 5, 6, 2, 3, 6, 5, 9, 5, 11, 7, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[the, grand, jury, commented, on, a, number, of, other, topics, ,, among, them, the, atlanta, and, fulton, county, purchasing, departments, which, it, said, ``, are, ...</td>\n",
              "      <td>[2, 5, 6, 105, 106, 69, 77, 11, 107, 108, 34, 109, 104, 2, 43, 41, 3, 4, 110, 111, 35, 81, 7, 17, 88, 112, 113, 41, 114, 115, 116, 117, 35, 118, 58, 2, 119, 76, 11, 1...</td>\n",
              "      <td>[DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, ADJ, NOUN, ., ADP, PRON, DET, NOUN, CONJ, NOUN, NOUN, VERB, NOUN, DET, PRON, VERB, ., VERB, ADV, VERB, CONJ, VERB, ADV, VE...</td>\n",
              "      <td>[2, 4, 3, 5, 6, 2, 3, 6, 4, 3, 7, 6, 11, 2, 3, 9, 3, 3, 5, 3, 2, 11, 5, 7, 5, 8, 5, 9, 5, 8, 5, 3, 2, 5, 6, 2, 4, 3, 6, 2, 3, 7, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[merger, proposed]</td>\n",
              "      <td>[122, 123]</td>\n",
              "      <td>[NOUN, VERB]</td>\n",
              "      <td>[3, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[however, ,, the, jury, said, it, believes, ``, these, two, offices, should, be, combined, to, achieve, greater, efficiency, and, reduce, the, cost, of, administratio...</td>\n",
              "      <td>[124, 34, 2, 6, 7, 81, 125, 17, 98, 126, 127, 128, 129, 130, 58, 131, 132, 133, 41, 134, 2, 135, 11, 136, 20, 26]</td>\n",
              "      <td>[ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, NUM, NOUN, VERB, VERB, VERB, PRT, VERB, ADJ, NOUN, CONJ, VERB, DET, NOUN, ADP, NOUN, ., .]</td>\n",
              "      <td>[8, 7, 2, 3, 5, 11, 5, 7, 2, 12, 3, 5, 5, 5, 10, 5, 4, 3, 9, 5, 2, 3, 6, 3, 7, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[the, city, purchasing, department, ,, the, jury, said, ,, ``, is, lacking, in, experienced, clerical, personnel, as, a, result, of, city, personnel, policies, '', .]</td>\n",
              "      <td>[2, 31, 110, 137, 34, 2, 6, 7, 34, 17, 138, 139, 28, 140, 141, 142, 143, 69, 144, 11, 31, 142, 145, 20, 26]</td>\n",
              "      <td>[DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, ., ., VERB, VERB, ADP, VERB, ADJ, NOUN, ADP, DET, NOUN, ADP, NOUN, NOUN, NOUN, ., .]</td>\n",
              "      <td>[2, 3, 5, 3, 7, 2, 3, 5, 7, 7, 5, 5, 6, 5, 4, 3, 6, 2, 3, 6, 3, 3, 3, 7, 7]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                          Tokenized_Sentence  ...                                                                                                                             Tag_Idxs\n",
              "0  [the, fulton, county, grand, jury, said, friday, an, investigation, of, atlanta's, recent, primary, election, produced, ``, no, evidence, '', that, any, irregularitie...  ...                                                          [2, 3, 3, 4, 3, 5, 3, 2, 3, 6, 3, 4, 3, 3, 5, 7, 2, 3, 7, 6, 2, 3, 5, 3, 7]\n",
              "1  [the, jury, further, said, in, term-end, presentments, that, the, city, executive, committee, ,, which, had, over-all, charge, of, the, election, ,, ``, deserves, the...  ...    [2, 3, 8, 5, 6, 3, 3, 6, 2, 3, 4, 3, 7, 2, 5, 4, 3, 6, 2, 3, 7, 7, 5, 2, 3, 9, 3, 6, 2, 3, 6, 3, 7, 6, 2, 3, 6, 2, 2, 3, 5, 5, 7]\n",
              "2  [the, september-october, term, jury, had, been, charged, by, fulton, superior, court, judge, durwood, pye, to, investigate, reports, of, possible, ``, irregularities,...  ...                           [2, 3, 3, 3, 5, 5, 5, 6, 3, 4, 3, 3, 3, 3, 10, 5, 3, 6, 4, 7, 3, 7, 6, 2, 4, 3, 2, 5, 5, 6, 3, 3, 3, 3, 7]\n",
              "3  [``, only, a, relative, handful, of, such, reports, was, received, '', ,, the, jury, said, ,, ``, considering, the, widespread, interest, in, the, election, ,, the, n...  ...                      [7, 8, 2, 4, 3, 6, 4, 3, 5, 5, 7, 7, 2, 3, 5, 7, 7, 6, 2, 4, 3, 6, 2, 3, 7, 2, 3, 6, 3, 9, 2, 3, 6, 2, 3, 7, 7]\n",
              "4            [the, jury, said, it, did, find, that, many, of, georgia's, registration, and, election, laws, ``, are, outmoded, or, inadequate, and, often, ambiguous, '', .]  ...                                                            [2, 3, 5, 11, 5, 5, 6, 4, 6, 3, 3, 9, 3, 3, 7, 5, 4, 9, 4, 9, 8, 4, 7, 7]\n",
              "5            [it, recommended, that, fulton, legislators, act, ``, to, have, these, laws, studied, and, revised, to, the, end, of, modernizing, and, improving, them, '', .]  ...                                                          [11, 5, 6, 3, 3, 5, 7, 10, 5, 2, 3, 5, 9, 5, 6, 2, 3, 6, 5, 9, 5, 11, 7, 7]\n",
              "6  [the, grand, jury, commented, on, a, number, of, other, topics, ,, among, them, the, atlanta, and, fulton, county, purchasing, departments, which, it, said, ``, are, ...  ...  [2, 4, 3, 5, 6, 2, 3, 6, 4, 3, 7, 6, 11, 2, 3, 9, 3, 3, 5, 3, 2, 11, 5, 7, 5, 8, 5, 9, 5, 8, 5, 3, 2, 5, 6, 2, 4, 3, 6, 2, 3, 7, 7]\n",
              "7                                                                                                                                                         [merger, proposed]  ...                                                                                                                               [3, 5]\n",
              "8  [however, ,, the, jury, said, it, believes, ``, these, two, offices, should, be, combined, to, achieve, greater, efficiency, and, reduce, the, cost, of, administratio...  ...                                                    [8, 7, 2, 3, 5, 11, 5, 7, 2, 12, 3, 5, 5, 5, 10, 5, 4, 3, 9, 5, 2, 3, 6, 3, 7, 7]\n",
              "9     [the, city, purchasing, department, ,, the, jury, said, ,, ``, is, lacking, in, experienced, clerical, personnel, as, a, result, of, city, personnel, policies, '', .]  ...                                                          [2, 3, 5, 3, 7, 2, 3, 5, 7, 7, 5, 5, 6, 5, 4, 3, 6, 2, 3, 6, 3, 3, 3, 7, 7]\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6m05_V6qM30",
        "colab_type": "code",
        "outputId": "637b9248-14b9-43f7-b326-418fa3413759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
        "    # Keras provides a convenient padding function; \n",
        "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
        "    return padded_idxs\n",
        "\n",
        "max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
        "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
        "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
        "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
        "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
        "#maximum length of the sentence is including 60 words\n",
        "#less than 60 has zero-paddings\n",
        "print(\"WORDS:\\n\", train_padded_words)\n",
        "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
        "\n",
        "print(\"TAGS:\\n\", train_padded_tags)\n",
        "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WORDS:\n",
            " [[  0   0   0 ...  24  25  26]\n",
            " [  0   0   0 ...  46  47  26]\n",
            " [  0   0   0 ...  66  67  26]\n",
            " ...\n",
            " [  0   0   0 ... 758  20  26]\n",
            " [  0   0   0 ... 802  34 447]\n",
            " [  0   0   0 ... 447 812  26]]\n",
            "SHAPE: (100, 60) \n",
            "\n",
            "TAGS:\n",
            " [[0 0 0 ... 5 3 7]\n",
            " [0 0 0 ... 5 5 7]\n",
            " [0 0 0 ... 3 3 7]\n",
            " ...\n",
            " [0 0 0 ... 3 7 7]\n",
            " [0 0 0 ... 3 7 3]\n",
            " [0 0 0 ... 3 3 7]]\n",
            "SHAPE: (100, 60) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JISH7UUqqM3-",
        "colab_type": "code",
        "outputId": "a697024a-a3e0-4e40-b26a-c8235667c6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "source": [
        "import numpy\n",
        "\n",
        "pandas.DataFrame(list(zip(train_sents['Tokenized_Sentence'].loc[0],\n",
        "                          [\"-\"] + train_sents['Tagged_Sentence'].loc[0],\n",
        "                          train_sents['Tagged_Sentence'].loc[0])),\n",
        "                 columns=['Input Word', 'Input Tag', 'Output Tag'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Word</th>\n",
              "      <th>Input Tag</th>\n",
              "      <th>Output Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>-</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fulton</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>county</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>grand</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jury</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>said</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>friday</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>an</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>investigation</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>of</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>atlanta's</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>recent</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>primary</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>election</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>produced</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>``</td>\n",
              "      <td>VERB</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>no</td>\n",
              "      <td>.</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>evidence</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>''</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>that</td>\n",
              "      <td>.</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>any</td>\n",
              "      <td>ADP</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>irregularities</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>took</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>place</td>\n",
              "      <td>VERB</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>.</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Input Word Input Tag Output Tag\n",
              "0              the         -        DET\n",
              "1           fulton       DET       NOUN\n",
              "2           county      NOUN       NOUN\n",
              "3            grand      NOUN        ADJ\n",
              "4             jury       ADJ       NOUN\n",
              "5             said      NOUN       VERB\n",
              "6           friday      VERB       NOUN\n",
              "7               an      NOUN        DET\n",
              "8    investigation       DET       NOUN\n",
              "9               of      NOUN        ADP\n",
              "10       atlanta's       ADP       NOUN\n",
              "11          recent      NOUN        ADJ\n",
              "12         primary       ADJ       NOUN\n",
              "13        election      NOUN       NOUN\n",
              "14        produced      NOUN       VERB\n",
              "15              ``      VERB          .\n",
              "16              no         .        DET\n",
              "17        evidence       DET       NOUN\n",
              "18              ''      NOUN          .\n",
              "19            that         .        ADP\n",
              "20             any       ADP        DET\n",
              "21  irregularities       DET       NOUN\n",
              "22            took      NOUN       VERB\n",
              "23           place      VERB       NOUN\n",
              "24               .      NOUN          ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdCxEAK1qM4I",
        "colab_type": "code",
        "outputId": "10031ca4-112f-4b36-bdaa-68cd7f7a51ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(pandas.DataFrame(list(zip(train_padded_words[0,1:], train_padded_tags[0,:-1], train_padded_tags[0, 1:])),\n",
        "                columns=['Input Words', 'Input Tags', 'Output Tags']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Input Words  Input Tags  Output Tags\n",
            "0             0           0            0\n",
            "1             0           0            0\n",
            "2             0           0            0\n",
            "3             0           0            0\n",
            "4             0           0            0\n",
            "5             0           0            0\n",
            "6             0           0            0\n",
            "7             0           0            0\n",
            "8             0           0            0\n",
            "9             0           0            0\n",
            "10            0           0            0\n",
            "11            0           0            0\n",
            "12            0           0            0\n",
            "13            0           0            0\n",
            "14            0           0            0\n",
            "15            0           0            0\n",
            "16            0           0            0\n",
            "17            0           0            0\n",
            "18            0           0            0\n",
            "19            0           0            0\n",
            "20            0           0            0\n",
            "21            0           0            0\n",
            "22            0           0            0\n",
            "23            0           0            0\n",
            "24            0           0            0\n",
            "25            0           0            0\n",
            "26            0           0            0\n",
            "27            0           0            0\n",
            "28            0           0            0\n",
            "29            0           0            0\n",
            "30            0           0            0\n",
            "31            0           0            0\n",
            "32            0           0            0\n",
            "33            0           0            0\n",
            "34            2           0            2\n",
            "35            3           2            3\n",
            "36            4           3            3\n",
            "37            5           3            4\n",
            "38            6           4            3\n",
            "39            7           3            5\n",
            "40            8           5            3\n",
            "41            9           3            2\n",
            "42           10           2            3\n",
            "43           11           3            6\n",
            "44           12           6            3\n",
            "45           13           3            4\n",
            "46           14           4            3\n",
            "47           15           3            3\n",
            "48           16           3            5\n",
            "49           17           5            7\n",
            "50           18           7            2\n",
            "51           19           2            3\n",
            "52           20           3            7\n",
            "53           21           7            6\n",
            "54           22           6            2\n",
            "55           23           2            3\n",
            "56           24           3            5\n",
            "57           25           5            3\n",
            "58           26           3            7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KtBGAWbqM4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Create the model'''\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import GRU\n",
        "\n",
        "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
        "                 n_tag_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=None):\n",
        "    #batch size = fitting NN일 때 쓴 sample data 크기. each of the NN fitting에 randomly chosen subsets of sample을 사용 \n",
        "    #              -> memory issue를 피함! (이 example에서는 10이라고 놨던 것 같습니다.)\n",
        "    \n",
        "    #stateful = False로 놓은 것 확인! Running RNN forgetting the data from the previous steps\n",
        "    \n",
        "    \n",
        "    #Layers 1\n",
        "    word_input = Input(batch_shape=(batch_size, seq_input_len), name='word_input_layer') #100*60같이\n",
        "    tag_input = Input(batch_shape=(batch_size, seq_input_len), name='tag_input_layer')\n",
        "\n",
        "    #Layers 2\n",
        "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
        "                                output_dim=n_word_embedding_nodes, \n",
        "                                mask_zero=True, name='word_embedding_layer')(word_input)  #mask_zero will ignore 0 padding\n",
        "                                #size(row number of embedding)는 columm numbers of input에서 갖고 오고, embedding의 column 개수는 알아서 설정. ex) 60*300 \n",
        "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
        "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
        "                               output_dim=n_tag_embedding_nodes,\n",
        "                               mask_zero=True, name='tag_embedding_layer')(tag_input) \n",
        "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
        "    \n",
        "    #Layer 3\n",
        "    merged_embeddings = Concatenate(axis=-1, name='concat_embedding_layer')([word_embeddings, tag_embeddings])\n",
        "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
        "    \n",
        "    #Layer 4\n",
        "    hidden_layer = GRU(units=n_hidden_nodes, return_sequences=True, \n",
        "                       stateful=stateful, name='hidden_layer')(merged_embeddings)\n",
        "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
        "    \n",
        "    #Layer 5\n",
        "    output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, \n",
        "                                         activation='softmax'), name='output_layer')(hidden_layer)\n",
        "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
        "    \n",
        "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
        "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", #crossentropy loss function을 사용\n",
        "                  optimizer='adam')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K8kjkXGqM4V",
        "colab_type": "code",
        "outputId": "d9aed944-3775-4b22-a7f5-ead9c36a777c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "model11 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=100,\n",
        "                     n_tag_embedding_nodes=100,\n",
        "                     n_hidden_nodes=500)\n",
        "model12 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=100,\n",
        "                     n_tag_embedding_nodes=200,\n",
        "                     n_hidden_nodes=500)\n",
        "model13 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=100,\n",
        "                     n_tag_embedding_nodes=300,\n",
        "                     n_hidden_nodes=500)\n",
        "model21 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=200,\n",
        "                     n_tag_embedding_nodes=100,\n",
        "                     n_hidden_nodes=500)\n",
        "model22 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=200,\n",
        "                     n_tag_embedding_nodes=200,\n",
        "                     n_hidden_nodes=500)\n",
        "model23 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=200,\n",
        "                     n_tag_embedding_nodes=300,\n",
        "                     n_hidden_nodes=500)\n",
        "model31 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=300,\n",
        "                     n_tag_embedding_nodes=100,\n",
        "                     n_hidden_nodes=500)\n",
        "model32 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=300,\n",
        "                     n_tag_embedding_nodes=200,\n",
        "                     n_hidden_nodes=500)\n",
        "model33 = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
        "                     # size of input data set(60) -> -1은 offset을 사용한 것. \n",
        "                     # offset: 여기서는 word that appeared only one time. trying to ignore this word.\n",
        "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
        "                     n_word_embedding_nodes=300,\n",
        "                     n_tag_embedding_nodes=300,\n",
        "                     n_hidden_nodes=500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-P73TEeqM4b",
        "colab_type": "text"
      },
      "source": [
        "### <font color='#6629b2'>Training</font>\n",
        "\n",
        "Now we're ready to train the model. We'll call the fit() function to train the model for 10 iterations through the dataset (epochs), using a batch size of 20 sentences. Keras reports to cross-entropy loss after each epoch, which should continue to decrease if the model is learning correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSJJU7O0qM4d",
        "colab_type": "code",
        "outputId": "a32be226-2b67-4582-b332-209004ea6278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train the model'''\n",
        "\n",
        "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
        "model11.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model11.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model11_weights.h5') #Save model\n",
        "\n",
        "model12.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model12.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model12_weights.h5') #Save model\n",
        "\n",
        "model13.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model13.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model13_weights.h5') #Save model\n",
        "\n",
        "model21.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model21.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model21_weights.h5') #Save model\n",
        "\n",
        "model22.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model22.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model22_weights.h5') #Save model\n",
        "\n",
        "model23.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model23.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model23_weights.h5') #Save model\n",
        "\n",
        "model31.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model31.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model31_weights.h5') #Save model\n",
        "\n",
        "model32.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model32.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model32_weights.h5') #Save model\n",
        "\n",
        "model33.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], \n",
        "          y=train_padded_tags[:, 1:, None], \n",
        "          batch_size=20, epochs=5)\n",
        "model33.save_weights('/content/gdrive/My Drive/Colab Notebooks/pos_tagging/example_model/model33_weights.h5') #Save model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.3555\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.2852\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.2266\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.1845\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.1511\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 2.4801\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 2.2177\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 2.0896\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.9653\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.8132\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 5s 47ms/step - loss: 2.4245\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 2.2148\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 2.0865\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 1.9542\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 1.7776\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 5s 46ms/step - loss: 2.4914\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 2.1877\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 2.0257\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.8721\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.7035\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 5s 50ms/step - loss: 2.4758\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 2.2060\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 2.0516\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 1.9043\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 1.6851\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 2.4095\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 2.0621\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 1.9128\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 1.7137\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 1.5018\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 2.4874\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 2.1744\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 1.9742\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 1.7670\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 1.5307\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 2.4450\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 2.0775\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 1.8780\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 1.6478\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 1.3709\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 2.4193\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 4s 39ms/step - loss: 2.1124\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 4s 38ms/step - loss: 1.9446\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 1.7238\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 1.4585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5wmN54Itc-A",
        "colab_type": "text"
      },
      "source": [
        "위의 결과를 보면 Epoch 5/5에서의 값으로 loss를 비교할 수 있다. \n",
        "그 결과, "
      ]
    }
  ]
}